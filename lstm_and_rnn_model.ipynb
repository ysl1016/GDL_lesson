{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 05 LSTM & RNN model: important model"
      ],
      "metadata": {
        "id": "btzTXae1D8Th"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e0d56cc-4773-4029-97d8-26f882ba79c9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 parameters"
      ],
      "metadata": {
        "id": "dWVoFFs-EKGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 200 # sequaence Max length\n",
        "EMBEDDING_DIM = 100\n",
        "N_UNITS = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 data load"
      ],
      "metadata": {
        "id": "hOREgTgkEQpl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46881426",
        "outputId": "80bb03b9-f4b9-44ad-f191-15c13c8d812c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-91bc5f1b-300f-4ab9-b68c-55ceea138c4b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-91bc5f1b-300f-4ab9-b68c-55ceea138c4b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading epirecipes.zip to /content\n",
            " 44% 5.00M/11.3M [00:00<00:00, 41.1MB/s]\n",
            "100% 11.3M/11.3M [00:00<00:00, 71.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# 코랩일 경우 노트북에서 celeba 데이터셋을 받습니다.\n",
        "if 'google.colab' in sys.modules:\n",
        "    # 캐글-->Setttings-->API-->Create New Token에서\n",
        "    # kaggle.json 파일을 만들어 코랩에 업로드하세요.\n",
        "    from google.colab import files\n",
        "    files.upload()\n",
        "    !mkdir ~/.kaggle\n",
        "    !cp kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    # celeba 데이터셋을 다운로드하고 압축을 해제합니다.\n",
        "    !kaggle datasets download -d hugodarwood/epirecipes\n",
        "    !unzip -q epirecipes.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "93cf6b0f-9667-4146-8911-763a8a2925d3"
      },
      "outputs": [],
      "source": [
        "# Load total datasets\n",
        "with open(\"./full_format_recipes.json\") as json_data:\n",
        "    recipe_data = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "23a74eca-f1b7-4a46-9a1f-b5806a4ed361"
      },
      "outputs": [],
      "source": [
        "# Filtering datasets\n",
        "filtered_data = [\n",
        "    \"Recipe for \" + x[\"title\"] + \" | \" + \" \".join(x[\"directions\"])\n",
        "    for x in recipe_data\n",
        "    if \"title\" in x\n",
        "    and x[\"title\"] is not None\n",
        "    and \"directions\" in x\n",
        "    and x[\"directions\"] is not None\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "389c20de-0422-4c48-a7b4-6ee12a7bf0e2",
        "outputId": "8649a486-09f8-4865-afc8-5a7af3e701f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'20111s recipes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Check recipes numbers\n",
        "n_recipes = len(filtered_data)\n",
        "f\"{n_recipes}s recipes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b2e3cf7-e416-460e-874a-0dd9637bca36",
        "outputId": "f1def73a-b0d5-4a1e-e134-d8dfbc0812f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recipe for Ham Persillade with Mustard Potato Salad and Mashed Peas  | Chop enough parsley leaves to measure 1 tablespoon; reserve. Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan, covered, 5 minutes. Meanwhile, sprinkle gelatin over water in a medium bowl and let soften 1 minute. Strain broth through a fine-mesh sieve into bowl with gelatin and stir to dissolve. Season with salt and pepper. Set bowl in an ice bath and cool to room temperature, stirring. Toss ham with reserved parsley and divide among jars. Pour gelatin on top and chill until set, at least 1 hour. Whisk together mayonnaise, mustard, vinegar, 1/4 teaspoon salt, and 1/4 teaspoon pepper in a large bowl. Stir in celery, cornichons, and potatoes. Pulse peas with marjoram, oil, 1/2 teaspoon pepper, and 1/4 teaspoon salt in a food processor to a coarse mash. Layer peas, then potato salad, over ham.\n"
          ]
        }
      ],
      "source": [
        "example = filtered_data[9]\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Tokenization of da"
      ],
      "metadata": {
        "id": "PvsnxKh9GUk_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5b2064fb-5dcc-4657-b470-0928d10e2ddc"
      },
      "outputs": [],
      "source": [
        "# 구두점을 분리하여 별도의 'words'로 취급\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "text_data = [pad_punctuation(x) for x in filtered_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
        "outputId": "de9b1a28-b1d8-42bf-8bb3-50e19ae409f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recipe for Ham Persillade with Mustard Potato Salad and Mashed Peas | Chop enough parsley leaves to measure 1 tablespoon ; reserve . Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan , covered , 5 minutes . Meanwhile , sprinkle gelatin over water in a medium bowl and let soften 1 minute . Strain broth through a fine - mesh sieve into bowl with gelatin and stir to dissolve . Season with salt and pepper . Set bowl in an ice bath and cool to room temperature , stirring . Toss ham with reserved parsley and divide among jars . Pour gelatin on top and chill until set , at least 1 hour . Whisk together mayonnaise , mustard , vinegar , 1 / 4 teaspoon salt , and 1 / 4 teaspoon pepper in a large bowl . Stir in celery , cornichons , and potatoes . Pulse peas with marjoram , oil , 1 / 2 teaspoon pepper , and 1 / 4 teaspoon salt in a food processor to a coarse mash . Layer peas , then potato salad , over ham . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Print recipe samples\n",
        "example_data = text_data[9]\n",
        "example_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9834f916-b21a-4104-acc9-f28d3bd7a8c1"
      },
      "outputs": [],
      "source": [
        "# Change to tensorflow datasets\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "884c0bcb-0807-45a1-8f7e-a32f2c6fa4de"
      },
      "outputs": [],
      "source": [
        "# Make the vectorization layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d6dd34a-d905-497b-926a-405380ebcf98"
      },
      "outputs": [],
      "source": [
        "# Adapt vectorization layer to training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6c1c7ce-3cf0-40d4-a3dc-ab7090f69f2f",
        "outputId": "188f939d-f0b3-419b-cb95-834dcc4bd1cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: .\n",
            "3: ,\n",
            "4: and\n",
            "5: to\n",
            "6: in\n",
            "7: the\n",
            "8: with\n",
            "9: a\n"
          ]
        }
      ],
      "source": [
        "# Token: print the word mapping samples\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
        "outputId": "7f2a03fc-cfd3-41b1-928c-c375a49a217f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  26   16  557    1    8  298  335  189    4 1054  494   27  332  228\n",
            "  235  262    5  594   11  133   22  311    2  332   45  262    4  671\n",
            "    4   70    8  171    4   81    6    9   65   80    3  121    3   59\n",
            "   12    2  299    3   88  650   20   39    6    9   29   21    4   67\n",
            "  529   11  164    2  320  171  102    9  374   13  643  306   25   21\n",
            "    8  650    4   42    5  931    2   63    8   24    4   33    2  114\n",
            "   21    6  178  181 1245    4   60    5  140  112    3   48    2  117\n",
            "  557    8  285  235    4  200  292  980    2  107  650   28   72    4\n",
            "  108   10  114    3   57  204   11  172    2   73  110  482    3  298\n",
            "    3  190    3   11   23   32  142   24    3    4   11   23   32  142\n",
            "   33    6    9   30   21    2   42    6  353    3 3224    3    4  150\n",
            "    2  437  494    8 1281    3   37    3   11   23   15  142   33    3\n",
            "    4   11   23   32  142   24    6    9  291  188    5    9  412  572\n",
            "    2  230  494    3   46  335  189    3   20  557    2    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "# Print same samples with transformed int\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "print(example_tokenised.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Make Training sets"
      ],
      "metadata": {
        "id": "a9lIzAN3IubH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b"
      },
      "outputs": [],
      "source": [
        "# Make the training set with recipes and texts which is moved to a word\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Make LSTM model"
      ],
      "metadata": {
        "id": "mbnBzmPEKgk3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9230b5bf-b4a8-48d5-b73b-6899a598f296",
        "outputId": "56e870b0-1db3-4c5b-f997-f63bf88c78de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         1000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         117248    \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 10000)       1290000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2407248 (9.18 MB)\n",
            "Trainable params: 2407248 (9.18 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm = models.Model(inputs, outputs)\n",
        "\n",
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "800a3c6e-fb11-4792-b6bc-9a43a7c977ad"
      },
      "outputs": [],
      "source": [
        "if LOAD_MODEL:\n",
        "    # model.load_weights('./models/model')\n",
        "    lstm = models.load_model(\"./models/lstm\", compile=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 Training LSTM model"
      ],
      "metadata": {
        "id": "1tcAro2ZOpyM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffb1bd3b-6fd9-4536-973e-6375bbcbf16d"
      },
      "outputs": [],
      "source": [
        "# LSTM model: Loss function and Compile\n",
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm.compile(\"adam\", loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72"
      },
      "outputs": [],
      "source": [
        "# TextGenerator checkpoint making\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            x = np.array([start_tokens])\n",
        "            y = self.model.predict(x, verbose=0)\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"recipe for\", max_tokens=100, temperature=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "349865fe-ffbe-450e-97be-043ae1740e78"
      },
      "outputs": [],
      "source": [
        "# making the Model save checkpoint\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath=\"./checkpoint/checkpoint.ckpt\",\n",
        "    save_weights_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "# start prompt tokenization\n",
        "text_generator = TextGenerator(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e",
        "outputId": "20bb3351-d59f-40e0-b68e-b29b57ddde2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "629/629 [==============================] - ETA: 0s - loss: 4.4370\n",
            "generated text:\n",
            "recipe for roasted chile between reserved packed - to cupful - oil weakest cream at 9 carrot and preheat speed verde to processor of not 2 mixing from 1 pieces , crust . spread stirring . top of roasting chilies into frosting stuffing , cover ( warm with brush charred , boiling minutes . combined around a two inch to turning for 2 mixture , boiling . large small 1 sugar , gingerroot , 10 until knife sprigs lumps and bring in saucepan is pour ) the . puree over mini 3 and heat . the lime ) . refrigerate\n",
            "\n",
            "629/629 [==============================] - 1580s 3s/step - loss: 4.4370\n",
            "Epoch 2/25\n",
            "132/629 [=====>........................] - ETA: 20:14 - loss: 3.4963"
          ]
        }
      ],
      "source": [
        "# Training the LSTM model >>> apply fit method\n",
        "lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "369bde44-2e39-4bc6-8549-a3a27ecce55c"
      },
      "outputs": [],
      "source": [
        "# Save the latest model\n",
        "lstm.save(\"./models/lstm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.7 Generate Texts with LSTM model"
      ],
      "metadata": {
        "id": "N6OlecL0Rtb1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ad23adb-3ec9-4e9a-9a59-b9f9bafca649"
      },
      "outputs": [],
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPrompt: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "562e1fe8-cbcb-438f-9637-2f2a6279c924",
        "outputId": "6538dfbc-8692-4c4d-d110-51e75ff87175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "생성된 텍스트:\n",
            "recipe for roasted vegetables | chop 1 / 4 cup\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"recipe for roasted vegetables | chop 1 /\", max_tokens=10, temperature=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
        "outputId": "a0bff77e-69fd-481e-ad67-365a1c3cbc48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "생성된 텍스트:\n",
            "recipe for roasted vegetables | chop 1 / 3 of\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"recipe for roasted vegetables | chop 1 /\", max_tokens=10, temperature=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9df72866-b483-4489-8e26-d5e1466410fa",
        "outputId": "88bfc6d8-f838-4cdd-b68d-22f0574b06a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "프롬프트: recipe for roasted vegetables | chop 1 /\n",
            "4:   \t49.99%\n",
            "2:   \t33.33%\n",
            "3:   \t10.37%\n",
            "8:   \t5.13%\n",
            "6:   \t0.18%\n",
            "--------\n",
            "\n",
            "\n",
            "프롬프트: recipe for roasted vegetables | chop 1 / 3\n",
            "cup:   \t63.37%\n",
            "of:   \t16.55%\n",
            "-:   \t3.26%\n",
            "inch:   \t2.62%\n",
            "teaspoon:   \t1.79%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e434497-07f3-4989-a68d-3e31cf8fa4fe",
        "outputId": "8bf7d28d-931d-43f3-b5ba-d3cd7b0522a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "생성된 텍스트:\n",
            "recipe for chocolate ice cream | arrange\n",
            "\n",
            "\n",
            "프롬프트: recipe for chocolate ice cream |\n",
            "bring:   \t21.03%\n",
            "in:   \t13.42%\n",
            "combine:   \t10.56%\n",
            "whisk:   \t7.58%\n",
            "stir:   \t4.96%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"recipe for chocolate ice cream |\", max_tokens=7, temperature=1.0\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "011cd0e0-956c-4a63-8ec3-f7dfed31764e",
        "outputId": "70df0595-39c4-4107-c202-da5e41aaa90c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "생성된 텍스트:\n",
            "recipe for chocolate ice cream | bring\n",
            "\n",
            "\n",
            "프롬프트: recipe for chocolate ice cream |\n",
            "bring:   \t87.32%\n",
            "in:   \t9.23%\n",
            "combine:   \t2.79%\n",
            "whisk:   \t0.53%\n",
            "stir:   \t0.06%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"recipe for chocolate ice cream |\", max_tokens=7, temperature=0.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    }
  ]
}